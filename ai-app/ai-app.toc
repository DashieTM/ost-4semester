\babel@toc {english}{}\relax 
\contentsline {section}{\numberline {1}CNN Convolutional Neural Networks}{1}{section.1}%
\contentsline {subsection}{\numberline {1.1}Keras}{1}{subsection.1.1}%
\contentsline {subsection}{\numberline {1.2}Flattening}{1}{subsection.1.2}%
\contentsline {section}{\numberline {2}Convolution}{1}{section.2}%
\contentsline {subsection}{\numberline {2.1}Firing of neurons}{1}{subsection.2.1}%
\contentsline {subsection}{\numberline {2.2}mathematical model of a feature detector}{1}{subsection.2.2}%
\contentsline {subsection}{\numberline {2.3}Example of convolution}{2}{subsection.2.3}%
\contentsline {subsection}{\numberline {2.4}Reasons for convolution}{2}{subsection.2.4}%
\contentsline {subsection}{\numberline {2.5}Stride}{2}{subsection.2.5}%
\contentsline {subsection}{\numberline {2.6}Padding}{2}{subsection.2.6}%
\contentsline {subsection}{\numberline {2.7}Max Pooling}{3}{subsection.2.7}%
\contentsline {subsection}{\numberline {2.8}Example with Keras}{3}{subsection.2.8}%
\contentsline {subsection}{\numberline {2.9}Softmax}{3}{subsection.2.9}%
\contentsline {subsection}{\numberline {2.10}Drop Out}{3}{subsection.2.10}%
\contentsline {subsection}{\numberline {2.11}Why?}{3}{subsection.2.11}%
\contentsline {subsection}{\numberline {2.12}Tensor}{4}{subsection.2.12}%
\contentsline {subsection}{\numberline {2.13}An example for image classification}{5}{subsection.2.13}%
\contentsline {section}{\numberline {3}Encoder and Decoder}{5}{section.3}%
\contentsline {subsubsection}{\numberline {3.0.1}Audio transformation}{5}{subsubsection.3.0.1}%
\contentsline {section}{\numberline {4}RNN}{6}{section.4}%
\contentsline {subsection}{\numberline {4.1}Sequential Data}{6}{subsection.4.1}%
\contentsline {subsection}{\numberline {4.2}Dealing with memory}{6}{subsection.4.2}%
\contentsline {subsection}{\numberline {4.3}The Solution}{6}{subsection.4.3}%
\contentsline {subsubsection}{\numberline {4.3.1}How do we handle the time component?}{6}{subsubsection.4.3.1}%
\contentsline {subsubsection}{\numberline {4.3.2}Adding Connections}{7}{subsubsection.4.3.2}%
\contentsline {subsubsection}{\numberline {4.3.3}Recurrence}{7}{subsubsection.4.3.3}%
\contentsline {subsubsection}{\numberline {4.3.4}Memory Cell}{7}{subsubsection.4.3.4}%
\contentsline {subsubsection}{\numberline {4.3.5}simpleRNNCell (keras)}{7}{subsubsection.4.3.5}%
\contentsline {subsubsection}{\numberline {4.3.6}Conputations across time/Forward Propagation}{7}{subsubsection.4.3.6}%
\contentsline {subsubsection}{\numberline {4.3.7}How to train RNN?}{8}{subsubsection.4.3.7}%
\contentsline {subsubsection}{\numberline {4.3.8}Loss across time steps}{8}{subsubsection.4.3.8}%
\contentsline {subsubsection}{\numberline {4.3.9}Backpropagation through Time}{8}{subsubsection.4.3.9}%
\contentsline {subsection}{\numberline {4.4}Deep RNN}{8}{subsection.4.4}%
\contentsline {subsubsection}{\numberline {4.4.1}Unroll}{9}{subsubsection.4.4.1}%
\contentsline {subsubsection}{\numberline {4.4.2}Bi-Directional RNN}{9}{subsubsection.4.4.2}%
\contentsline {subsubsection}{\numberline {4.4.3}Backpropagation through time BPTT}{9}{subsubsection.4.4.3}%
\contentsline {subsubsection}{\numberline {4.4.4}Vanishing gradient of simple/vanilla RNNs}{9}{subsubsection.4.4.4}%
\contentsline {subsubsection}{\numberline {4.4.5}Solutions to the problem of backpropagation}{9}{subsubsection.4.4.5}%
\contentsline {subsubsection}{\numberline {4.4.6}Efficient Solution: Gated RNNs}{10}{subsubsection.4.4.6}%
\contentsline {subsubsection}{\numberline {4.4.7}LSTM}{10}{subsubsection.4.4.7}%
\contentsline {subsubsection}{\numberline {4.4.8}Gated Recurrent Units GRU}{12}{subsubsection.4.4.8}%
\contentsline {subsubsection}{\numberline {4.4.9}LSTM vs GRU}{13}{subsubsection.4.4.9}%
\contentsline {subsection}{\numberline {4.5}Seq2seq}{13}{subsection.4.5}%
\contentsline {subsubsection}{\numberline {4.5.1}Encoder}{14}{subsubsection.4.5.1}%
\contentsline {subsubsection}{\numberline {4.5.2}Decoder}{14}{subsubsection.4.5.2}%
\contentsline {subsubsection}{\numberline {4.5.3}Problems}{14}{subsubsection.4.5.3}%
\contentsline {subsection}{\numberline {4.6}Transformers}{14}{subsection.4.6}%
\contentsline {subsubsection}{\numberline {4.6.1}Attention}{14}{subsubsection.4.6.1}%
\contentsline {section}{\numberline {5}Reinforcement Learning}{16}{section.5}%
\contentsline {subsection}{\numberline {5.1}Limitations}{16}{subsection.5.1}%
\contentsline {subsection}{\numberline {5.2}Markov Decision Process MDP}{16}{subsection.5.2}%
\contentsline {subsection}{\numberline {5.3}Discount factory \(\gamma \)}{16}{subsection.5.3}%
\contentsline {subsection}{\numberline {5.4}Return R vs Reward G}{16}{subsection.5.4}%
\contentsline {subsection}{\numberline {5.5}The State Value Function V(s)}{16}{subsection.5.5}%
\contentsline {subsubsection}{\numberline {5.5.1}Temporal Difference}{17}{subsubsection.5.5.1}%
\contentsline {subsection}{\numberline {5.6}Policy Evaluation: Estmating state values V(s)}{17}{subsection.5.6}%
\providecommand \tocbasic@end@toc@file {}\tocbasic@end@toc@file 
